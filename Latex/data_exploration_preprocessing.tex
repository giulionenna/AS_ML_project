We first import the dataset and encode each categorical feature using \textit{one-hot encoding}. This means that numerical features will be left unchanged while for each categorical feature the process is the following:
\begin{description}
    \item[1.] Determine all the distinct values of that feature (categories)
    \item[2.] For each category generate a new binary column
    \item[3.] Assign values to the binary columns according to categories featred in each line.   
\end{description}
\begin{center}
    \begin{tabular}{|c|}
        \hline
        feature \\
        \hline
        \hline
        category 1 \\
        \hline
        category 2 \\
        \hline
        category 3 \\
        \hline
        category 2 \\
        \hline
    \end{tabular}
    \quad
    \begin{tabular}{|c|c|c|}
        \hline
        category 1 & category 2 & category 3\\
        \hline
        \hline
        1 & 0 & 0 \\
        \hline
        0 & 1 & 0 \\
        \hline 
        0 & 0 & 1 \\
        \hline
        0 & 1 & 0 \\
        \hline
    \end{tabular}    
\end{center}
\begin{lstlisting}[language=Python, caption= Data encoding]
    import pandas as pd

    df = pd.read_csv('Data/bank/bank-additional-full.csv', sep=';')
    display(df.head())
    cat_col = df.dtypes=='O'
    df_enc = pd.get_dummies(df.loc[:, cat_col], prefix=df.columns[cat_col])
    df_enc = df_enc.join(df.loc[:, np.logical_not(cat_col)])

    df_enc = df_enc.drop('y_no', axis=1) #deleting column since attribute "y" is binary
    df_enc = df_enc.drop('duration', axis = 1) #drop the duration column (see attribute information)

\end{lstlisting}

\subsection{Data Exploration}
In order to gain useful insights about some of the attributes featured in the dataset we can now perfom some \textit{data exploration}. This step of the analysis is qualitative by nature and consists in plotting graphs and distributions relative to each feature in the dataset.\\
\\
 There are 11 categorical features and the remaining are numerical. Categorical data will be explored by means of \textit{pie charts} while numerical data will be explored by means of a \textit{scatterplot matrix}

\begin{figure}[H]
    \centering
    \subfloat[1][Pieplot relative to \texttt{job}]{\includegraphics[scale = 0.8]{pictures/pieplot0_job.pdf}}
    \qquad
    \qquad
    \subfloat[2][Pieplot relative to \texttt{marital}]{\includegraphics[scale = 0.8]{pictures/pieplot1_marital.pdf}}
\end{figure}

\begin{figure}[H]
    \centering
    \subfloat[1][Pieplot relative to \texttt{education}]{\includegraphics[scale = 0.8]{pictures/pieplot2_education.pdf}}
    \qquad
    \qquad
    \subfloat[2][Pieplot relative to \texttt{default}]{\includegraphics[scale = 0.8]{pictures/pieplot3_default.pdf}}
\end{figure}

\begin{figure}[H]
    \centering
    \subfloat[1][Pieplot relative to \texttt{housing}]{\includegraphics[scale = 0.8]{pictures/pieplot4_housing.pdf}}
    \qquad
    \qquad
    \subfloat[2][Pieplot relative to \texttt{loan}]{\includegraphics[scale = 0.8]{pictures/pieplot5_loan.pdf}}
\end{figure}

\begin{figure}[H]
    \centering
    \subfloat[1][Pieplot relative to \texttt{contact}]{\includegraphics[scale = 0.8]{pictures/pieplot6_contact.pdf}}
    \qquad
    \qquad
    \subfloat[2][Pieplot relative to \texttt{month}]{\includegraphics[scale = 0.8]{pictures/pieplot7_month.pdf}}
\end{figure}
\begin{figure}[H]
    \centering
    \subfloat[1][Pieplot relative to \texttt{day\_of\_the\_week}]{\includegraphics[scale = 0.8]{pictures/pieplot8_day_of_week.pdf}}
    \qquad
    \qquad
    \subfloat[2][Pieplot relative to \texttt{poutcome}]{\includegraphics[scale = 0.8]{pictures/pieplot9_poutcome.pdf}}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.55]{pictures/scatterplot.pdf}
    \caption{Scatterplot matrix of numerical features }
\end{figure}
From the scatterplot we don't see any particular correlation between numerical features. Yet, as for the coloring of the data, one can notice that the attribute \texttt{campaign} discriminates pretty well the outcome. As a matter of fact people that recieved a lot of calls from the call center never buy the product that the bank is trying to sell. This information alone could be of some value for reducing cost and better targeting calls.

\subsection{Dataset splitting}
After exploring data, the first thing to do is to perform a split of our dataset into training and test set.
\begin{lstlisting}[language=Python, caption= Data splitting]
    df_train, df_test = train_test_split(df_enc, test_size=0.1, random_state=42)
\end{lstlisting}
As it is the case with some binary classification datasets, our data is heavly biased towards one class of outcome. In our case the most common outcome for each call is, reasonably, unsuccesful call: meaning that the client that has been called have \textbf{not} purchased the product that the bank is trying to sell. In order to quantify the class unbalance we can use a simple pie chart:

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{pictures/pieplot_unbalance.pdf}

    \caption{Responce variable class unbalance}
\end{figure}

As we can see $\sim 90\%$ of our data is biased towards the majority class (no). If we tried to feed the data as it is to any classification model the resulting classifier would seem to be "accurate" even if it really isn't.\\
\\
As a simple example we could think of a classifier that classifies every data entry as "no" no matter of the features. This naive classifier, tested with our heavily biased data, would result to have $\sim 90\%$ accuracy, the same percentage of the majority class with respect to the whole dataset.\\
\\
Various techniques can be used in order to solve this problem, among them there are \textbf{Oversampling} and \textbf{Undersampling}. 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{pictures/undersampling-oversampling.png}

    \caption*{Visual exapmple of Oversampling and Undersampling}
\end{figure}
As the name suggests, oversampling takes place when the minority class is used to generate new bootstrapped data to balance classess. On the other hand undersampling consists in randomly sampling from the majority class enough samples such that the two classes are balanced. In our case, since we have enough data, we can opt for undersampling in order to balance classes.

\begin{lstlisting}[language=Python, caption= Data rebalancing]
response = df_train['y_yes']
feature_matrix = df_train.loc[:, df_train.columns!='y_yes']

#rebalancing classes (y_yes=1 is the minority class)--------
ratio = df_train['y_yes'].sum()/df_train['y_yes'].size
print("Original training dataset has", df_train.index.size, "Observations. \n", "Among them", response.sum(), "are positive and", response.size-response.sum(), "are negative \n Performing dataset rebalancing by undersampling... \n\n" )
df_train_false = df_train.loc[response==0, :]
df_train_false_resampled = df_train_false.sample(frac=ratio)

df_train_true = df_train.loc[response==1,:]

df_train = pd.concat([df_train_true, df_train_false_resampled])
ratio = df_train['y_yes'].sum()/df_train['y_yes'].size

response = df_train['y_yes']
feature_matrix = df_train.loc[:, df_enc.columns!='y_yes']

print("Training Dataset rebalancing performed. Dataset has now", df_train.index.size, "Observations. \n", "Among them", response.sum(), "are positive and", response.size-response.sum(), "are negative \n\n" )

X_train = df_train.loc[:, df_train.columns!='y_yes']
X_test = df_test.loc[:, df_test.columns!='y_yes']
y_train = df_train['y_yes']
y_test = df_test['y_yes']

\end{lstlisting}
One can notice that only the training tast was subject to the rebalancing process. This is because real-world data will not be balanced and rebalancing test data would cause the same problems concearning test accuracy.

\subsection{Scaling the dataset}

The encoded dataset is characterized by features with very different scale/range  of values. In this case, if we apply PCA or any kind of training model on this data, some important features can be "covered" by other feature just for the unit measure disparity. This is why is a good practice to \textbf{rescale} the data. The most used scalers used are the \textbf{Min-max Scaler} and \textbf{Standard Scaler}.\\
\\
The \textbf{Min-Max Scaler} trasnforms the data such that the values of each column are distributed between a minimum value $m$ and a maximum value $M$. Let $X \in \mathbb{R}^{n \times p}$ be our feature matrix, with $X=[x_1, x_2, \dots, x_p]$. Let $X' \in \mathbb{R}^{n \times p}$ be our scaled matrix with $X'=[x'_1, x'_2, \dots, x'_p]$. If $M_i = \max\{x_i\}$, $m_i = \min\{x_i\}$, $i \in \{1,  \dots,  p\}$ then
\begin{equation}
    x'_i = \frac{x_i - m_i}{M_i-m_i} \quad i \in \{1,  \dots,  p\}    
\end{equation}
where all operations are intended to be element-wise. \\
\\
On the other hand the \textbf{Standard-Scaler} makes the values of each feature in the data to have zero-mean and unit-variance. In particular: 
\begin{equation}
    x'_i = \frac{x_i - \bar{x_i}}{s_i} \quad i \in \{1,  \dots,  p\}    
\end{equation}
where $\bar{x_i}$ is the \textit{sample mean} and $s_i$ is the \textit{sample variance} of the $i$-th column
\begin{lstlisting}[language=Python, caption= Data scaling]
#Performing scaling using min-max scaler
scaler_mm = MinMaxScaler()
scaler_mm.fit(X_train)

X_train_scaled_mm = scaler_mm.transform(X_train)
X_test_scaled_mm = scaler_mm.transform(X_test)

#Performing scaling using standard scaler
scaler_std = StandardScaler()
scaler_std.fit(X_train)

X_train_scaled_std = scaler_std.transform(X_train)
X_test_scaled_std = scaler_std.transform(X_test)
\end{lstlisting}
As it is the case with most steps in our analysis pipeline, scaling is \textit{fitted} on \textbf{training data}, meaning that all the values used for the scaling are computed on the training dataset. When performing a prediction, test data should also undergo the same rescaling process using however the values fitted with the training data. Not doing so would result in what in data analysis is called \textit{data leakage}, meaning that information about test data \textit{leaked} into the training process.

\subsection{Performing PCA on the dataset}
PCA stands for \textit{principal component analysis} and is a technique used to reduce the dimensionality of the data (number of columns) while retaining as much information as possible.