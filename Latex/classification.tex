What will follow is a brief theoretical overview of all the classification models used on the data, with the objective of classifing if a call will be successful or not w.r.t. the features provided to the model.

\subsection{From Decision Trees to Random Forests}
\subsubsection*{Decision Trees}
Let \(\chi\) be our feature space (meaning the space in which features live in, e.g. \(\chi = \mathbb{R}^p\)). The main idea behind a decision tree is to partition the feature space such that data points will be classified according to the partition to which they belong. \\
\\
This method is called decision \textit{tree} because it can be seen as a binary tree that mimics the splitting process of the feature space where each node \(\nu\) represents a (sub)split of the space and each leaf \(\omega\) represents a subset \(R_\omega \subseteq \chi\). Hence each leaf of the tree is associated to a classifier \(g^\omega\) that (in the classification case) gives as output the class relative to that particular partition of the feature space.
\\
\\
Let \(\tau = \{x_i, y_i \}_{i = 1, \dots ,n}\) be our training set, then our classifier will look like:
\begin{equation}
    g(x) =  \sum \limits_{\omega}^{} g^\omega(x)\mathbbm{1}\{x \in R_\omega\}
\end{equation}
And our loss function can be written as:
\begin{equation}
    l_\tau(g) = \frac{1}{n}  \sum \limits_{i= 0}^{n}\text{Loss}(y_i, g(x_i)) =  \sum \limits_{\omega}^{} \frac{1}{n} \underbrace{\sum \limits_{i=1}^{n} \mathbbm{1}\{x_i  \in R_\omega \} \text{Loss}(y_i, g^\omega(x_i))}_{\text{Loss contribution of each leaf }\omega}
\end{equation}
\\
This means that \textbf{total loss can be seen as the sum of the loss on each region (leaf) of the feature space}. Hence, since minimizing loss by finding the optimal partitioning would be an \textit{np-hard} problem, we can formulate a \textit{greedy} algorithm that tries to split recursively our feature space trying to minimize loss at each step.
\\
\\
Let \(\nu\) be an internal node of a decision tree. For each node we can assign a region \(R_\nu \subseteq \chi \) and a subset of the training  set containing points that live in that region \(\sigma_\nu = \{(x, y) \in \tau
\text{:}  x \in R^\nu\}\). To each node is associated a splitting rule \(s\) that can define two subset of the training set:
\begin{eqnarray*}
    \sigma_T = \{(x, y) \in \sigma \text{ s.t } s(x) = \texttt{True}\}\\
    \sigma_F = \{(x, y) \in \sigma \text{ s.t } s(x) = \texttt{False}\}
\end{eqnarray*}
\\
Hence we can define the following algorithm:
\begin{algorithm}[H]\caption{Greedy Decision Tree}
    \label{algo:greedy}
    \begin{algorithmic}
    \State Imput: Tree node \(\nu\), \(\sigma_\nu \subset \tau\)
    \If{The terminal condition is met(\(\nu\) is a leaf)}
    \State define \(g^\nu = \arg\max_{z \in \{0, \dots c- 1\}} = \frac{1}{| \sigma_\nu |}  \sum \limits_{(x_i, y_i) \in R_\nu}^{} \mathbbm{1}\{y_i = z\} \)
    \EndIf
    \State Search for the best splitting rule \(s\)
    \State Generate \(\sigma_T\) and \(\sigma_F\)
    \State Generate \(\nu_T\) and \(\nu_F\)
    \State \(\mathbb{T}_{\nu_T} \gets \texttt{Greedy Decision Tree}(\nu_T, \sigma_T)\) (Left subtree)
    \State \(\mathbb{T}_{\nu_F} \gets \texttt{Greedy Decision Tree}(\nu_F, \sigma_F)\) (Right subtree)\\
    \Return \(\mathbb{T}_\nu\) (subtree starting from \(\nu\))
    \end{algorithmic}            
\end{algorithm}

\textbf{But how the best splitting rule is defined?} Given a node \(\nu\) and the relative \(\sigma_\nu \subseteq \tau\), since the loss is additive with respect to regions of \(\chi\), we can compute the contribution of \(\nu\) to the loss:
\begin{equation}
    \label{loss_not_splitted}
    \frac{1}{n}  \sum \limits_{i= 1}^{n} \mathbbm{1}\{x_i \in \sigma_\nu\}\text{Loss}(y_i, g^\nu(x_i))
\end{equation}
Alternatively we can compute the contribution of \(\nu\) to the loss if splitted into \(\sigma_T\) and \(\sigma_F\)
\begin{equation}
    \label{loss_splitted}
    \frac{1}{n}  \sum \limits_{i= 1}^{n} \mathbbm{1}\{x_i \in \sigma_T\}\text{Loss}(y_i, g^T(x_i))\: +\: \frac{1}{n}  \sum \limits_{i= 1}^{n} \mathbbm{1}\{x_i \in \sigma_F\}\text{Loss}(y_i, g^F(x_i))
\end{equation}
Where \(g^T\) and \(g^F\) computed in a \textit{greedy way}, meaning that \(\sigma_T\) and \(\sigma_F\) are seen as leaves. Since \ref{loss_splitted} is always less or equal than \ref{loss_not_splitted}, in trying to find the best splitting rule our objective is to minimize \ref{loss_splitted}. Difference between the two quantities can be used in the terminal condition: if splitting the node \(\nu\) does not produce a sensible reduction in loss then the algorithm can stop. 
\\
\\
Since ours is a classification task, \ref{loss_splitted} can be written as:
\begin{equation}
    \frac{1}{|\sigma_T|}  \sum \limits_{(x_i, y_i) \in \sigma_T}^{} \mathbbm{1} \{y_i \neq y_T^*\}\: +\: \frac{1}{|\sigma_F|}  \sum \limits_{(x_i, y_i) \in \sigma_F}^{} \mathbbm{1} \{y_i \neq y_F^*\}
\end{equation}
Where \(y_T^*\) and \(y_F^*\) are the majority classes in \(\sigma_T\) and \(\sigma_F\) respectively. Morover, since:
\begin{equation}
    \frac{1}{|\sigma_T|}  \sum \limits_{(x_i, y_i) \in \sigma_T}^{} \mathbbm{1} \{y_i \neq y_T^*\} = 1 - \frac{1}{|\sigma_T|}  \sum \limits_{(x_i, y_i) \in \sigma_T}^{} \mathbbm{1} \{y_i = y_T^*\} = 1- p_z^{\sigma_T}
\end{equation}
Where \(p_z^{\sigma_T}\) is the ratio of the majoirty class in \(\sigma_T\) relative to the other classes and can be seen as an \textbf{Impurity index}. The minimization of the loss at each split can thus be seen as the minimization of an impority index. Hence various impurity indexes can be used such as Entropy impurity, and the Gini index.
\subsubsection*{Random Forests}
Assume we could have an ensamble of training sets \(\tau_1, \dots , \tau_B\) indipendent and identically distributed. Let \(g_{\tau_b}\) the classification trees modeled on each training set. Then:
\begin{equation}
    \label{majority_classifier}
    g_{\text{maj}}(x) = \argmax_{z \in \{0, \dots, c-1\}}  \sum \limits_{b= 1}^{B} \mathbbm{1}\{g_{\tau_b}(x) = z\}
\end{equation} 
would be the result of a classification function that is the result of a "voting mechanism" where each \(g_{\tau_b}\) contributes to the vote and the majority class would win. The more independent training sets we have, the more the resulting \(g_{\text{maj}}\) would be "wise". Obviously in a real world envoirment we would never have such luxury of having i.i.d. training sets, hence we have to resort to \textbf{Bootstrapping}, meaning that from one training set \(\tau\) we generate \(B\) training sets \(\tau^*_1, \dots , \tau^*_B\) sampling from \(\tau\). \\
\\
Depending on the dimensions of the original dataset, using bootstrapping makes us loose independency between training sets. Hence trees generated from each training set will be "correlated\footnote{Here the word \textit{correlation} is slightly misused since we do not actually mean correlation in a statistical sense buy more in an heuristical sense.}" meaning that they would be similar since they are generated basically from the same data. In order to \textit{decorrelate} trees generated from \(\tau^*_1, \dots , \tau^*_B\) we use a simple trick: \textbf{at each split of the tree we only consider a subset of \(m< p\) features instead of the full feature set.}
\\
\\
This simple trick will generate "randomized" trees that are "decorrelated" from each other. After generating \(\{g_{\tau^*_b}\}_{b = 1, \dots B}\) random trees we can consider the "majority classifier" as seen in \ref{majority_classifier}: we have generated a \textbf{Random Forest.}

\subsection{Boosting}
The practice of \textbf{Boosting}, as the name implies, is an iterative method used to boost the performances of a \textit{Weak learner} by generating other weak learners. \\
\\
We call a learner \textit{Weak} if it is a classification/regression model that can be trained very easily at the expence of accuracy (e.g. a decision tree with depth equal to 2 is a weak learner).\\
\\
Let \(g_0\) be a weak learner, then our objective is to find another weak learner \(h_1 \in \mathcal{H}\) such that:
\begin{equation}
    h_1 = \argmin_{h_1 \in \mathcal{H}} \frac{1}{n}  \sum \limits_{(x_i, y_i) \in \tau}^{} \text{loss}(g_0(x_i) + \gamma h_1(x_i), y_i) 
\end{equation}
Once we find \(h_1\) we can define \(g_1 = g_0 + h_1\) and by the same token we can find \(h_2\) and so on. Hence after \(B\) boosting steps we obtain:
\begin{equation}
    g_B(x) = g_0(x) +  \sum \limits_{i= 1}^{B} \gamma h_i(x)
\end{equation}
In a classification setting, where \(g_0(x) \in \{- 1, + 1\}\) and \(h_i(x) \in \{- 1, + 1\}, i= 1\dots B\) then:
\begin{equation}
    g_B(x) = \sign \left[g_0(x) +  \sum \limits_{i= 1}^{B} \gamma_i h_i(x)\right]
\end{equation}
To compute \(g_B(x)\) we can use the \textbf{AdaBoost algorithm} that deploys an "adaptive" boosting algorithm by wheighting the loss on the points. This algorithm arises from defining \(\text{Loss}(y, \hat{y}) = e^{y\hat{y}}\). 
\subsection{Logistic Regression}
Let \(Y_i\) be the random variable associated with the responce \(y_i\) of the i-th data entry (successful/unsuccessful call). Assume \(Y_i\) has a \textit{Bernoulli} distribution:
\begin{equation}
    Y_i \sim \text{Ber}(\pi_i)
\end{equation}
Using the \textbf{Generalized Linear Models} framework, we assume that \(\pi_i\) is a function of a linear combination of the parameters:
\begin{equation}
    \pi_i = \mathbb{E}[Y_i] = h(x_i^T\beta) = \frac{1}{1- e^{- x_i^T\beta}}.
\end{equation}
As in linear regression, we can find a suitable \(\beta\) by maximizing the \textit{log-likelihood} function of our data. After some computation we obtain:
\begin{equation}
    -\frac{1}{n} \log[f(y|\beta,X)] = \frac{1}{n}  \sum \limits_{i= 1}^{n} (1- y_i) x_i^T \beta + \log(1+ e^{- x_i^T\beta})
\end{equation}
Which is a \textit{\textbf{convex} minimization problem}, thus easy to solve. In solving this minimization we can also introduce a \textit{regularization argument} using \(l_1\), \(l_2\) norm or both on \(\beta\).\\
After computing the optimal \(\hat{\beta}\), we can then deploy our classificator which is based on computing the probability of each outcome (\(y_i\) successful/unsuccessful)
\begin{equation}
    g_\tau(x_i) = \argmax_{y \in \{0, 1\}} \big[ (h(x_i^T\hat{\beta}))^{y}(1-h(x_i^T\hat{\beta}))^{1- y} \big] 
\end{equation}

\subsection{K-Nearest Neighbors Classification}
K-Nearest Neighborst classification makes use of a distance metric in order to compute, as the name suggests, the \(k\) nearest neighbors of a given data point \(x\). Thus, given \(x\) we could define the set of the \(K\) nearest neighbors of \(x\) according to the chosen metric. 
\begin{equation*}
    \tau(x) =\{ (x_{(1)}, y_{(1)}), \dots (x_{(K)}, y_{(K)})\}
\end{equation*}
Then the \textit{K-nearest neighbors} classification rule classifies \(x\) according to the most frequently occurring class labels in \(\tau(x)\).